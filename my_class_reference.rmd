---
title: "Class Reference"
author: "Derek Willis"
output:
  html_document:
    theme: cerulean
    highlight: pygments
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Consider this a personal guide to the commands and functions you will learn. In general, when you come across an R command or function that you want to remember, put it in here along with a description of what it does and when you'd use it.

### How to set the working directory

The command to set a working directory is setwd(). For example, this sets the working directory to a directory called data_journalism_2024_fall inside a directory called jour472 located inside your home directory (the \~ is equivalent of /Users/[youruser] on a Mac).


### How to install and load the tidyverse

```{r}
install.packages("tidyverse")
library(tidyverse)
```


```{r}

#print will take a string and return it in the terminal
print("hello world")

```

------------------------------------------------

#pre_lab_01 notes#
#R basics#
# typing 2+2 into the console, then hitting enter will spit out number 4, function being it can compute equations and provide useful solutions. example seen below in console. however won't replicate into page, only console which is downside if want to continuously run and fix errors.#

ex.
```{r}
2+2
```

#variables can be assigned by using left facing arros <--, called assignment operator, can be seen in consol example. can have as many variables as can be named; reusable; anything can be stored --whole table, list of numbers, single word, whole book etc.#

```{r}
number <-- 2
```

```{r}
firstnumber <-- 1
secondnumber <-- 2
```

```{r}
(firstnumber + secondnumber) * secondnumber
```

#external libraries: "packages"; used to solve very specific problems -- example, Tidyverse. can install packages with function,, install.packages(),, and only have to do once

```{r}
install.packages('tidyverse')
```


#
In Rmarkdown data notebooks, code is written inside of codeblocks, and explanatory text in the white area outside of it.
```{r}
# This is a comment inside of a codeblock. Comments doesn't run when we run the codeblock.
# Adding this command is a good way to start your class reference notebook.
library(tidyverse)
```


#Aggregates#
#CSV: stripped down version of spreadsheet; each column is spearated by a comma#
#RDS files: less common when getting data from elsewhere/others#

RDS example: 
```{r}
umd_courses <- read_rds("data/umd_courses.rds")
```

#umd_courses: name of variable#
#read_rds(): function, only works when tidyverse is loaded; function takes parts of computer code that takes info and follows a series of pre-determined steps and shoots it back out#
#inside read_rds() is arguent: customize what function does; in this data set, each row represent course offered at UMD by departments during specific terms etc.#

#when use data sets/bases: looking into environment --> see number of rows ("obs" short for observations), number of columns (variables); can double click on dataframe name in envi. and explore like spreadsheet#

```{r}
glimpse(umd_courses)
```
#^^will provide list of columns, data type for each column and first few values for each column#

```{r}
head(umd_courses)
```
#^^print out columns and first six rows of data#

#package dplyr has function that can take massive amounts of data and analyze it; grouping like things together, doing simple things like counting them, averaging them together etc.#
#to do this, take dataset and introduce new operator |> "and then do this" type of function: called "pipe operator"; (shortcut for it: ctrl-shift-m)#
#can also use %>% --> same thing#

#data |> function: basically take the data set and then do this to it (commonly done/used)#

#very common pattern when using data sets:
data |> group_by(COLUMN NAME) |> summarize(VARIABLE NAME = AGGREGATE FUNCTION(COLUMN NAME))#

#in dataset, column with term information called "term"#

#code to count number of courses in each term:

```{r}
umd_courses |>
  group_by(term) |>
  summarise(
    count_classes = n()
  )
```
dataset -- umd_course --
name -- term -- means group together all terms; which could be gotten by using glimpse()
after group data, need to count them up -- summarize()
inside summarize() needs to be the summaries one wants to find, hence putting -- count_classes = n()
-- 
count_classes: creating new field
setting equal to n(): fuction that counts number rows or records in each group#

#but this once run, aren't in order, so add |> arrange() to put data in order; default ascending order smallest - largest#

```{r}
umd_courses |>
  group_by(term) |>
  summarise(
    count_classes = n()
  ) |>
  arrange(desc(count_classes))
```

#can even group more than one thing#

```{r}
umd_courses |>
  group_by(term, department) |>
  summarise(
    count_classes = n()
  ) |>
  arrange(term, department)
```

#sum(): know total number of data grouped; add up all the values in the column 
ex. to see total number of seats --> sum(seats)#
ex.
```{r}
umd_courses |>
  group_by(term) |>
  summarise(
    count_classes = n(),
    total_seats = sum(seats)
  ) |>
  arrange(desc(total_seats))
```

#can also calculate average number of seats (the mean) and midpoint of data (the median)#
ex. 
```{r}
umd_courses |>
  group_by(term) |>
  summarise(
    count_classes = n(),
    total_seats = sum(seats),
    mean_seats = mean(seats),
    median_seats = median(seats)
  ) |>
  arrange(desc(total_seats))
```

#highest and lowest points in data are, respectively, min() and max()#
ex.
```{r}
umd_courses |>
  group_by(department) |>
  summarise(
    count_classes = n(),
    total_seats = sum(seats),
    mean_seats = mean(seats),
    median_seats = median(seats),
    min_seats = min(seats),
    max_seats = max(seats)
  ) |>
  arrange(desc(total_seats))
```

#to sort from highest to lowest on a specific data point#
ex. 
```{r}
umd_courses |>
  arrange(desc(seats))
```


#n_designate can be used to help...


Chapter 18: Mutating Data
#can obtain percentages using -- dplyr and mutate -- to calcualte new metrics in new field using existing fields of data#

#essence of mutate --> using data you have to answer a new question#

#code to calculate percentages: with -summarize- n() is used to count things --> with mutate, a very similar syntax is used to calculate new value -- a new column of data -- using other values in dataset: to calculate percentage, need both the number and total number#

```{r}
general_22 <- general_22 |>
  mutate(
    total_votes = cox + moore + lashar + wallace + write_ins,
    pct_moore = moore/total_votes
  )
```

#however this makes a decimal expressed as a percentage --> to fix! multiply by 100: since replacing contents of new pct_moore column, can just update previous code and run it again: 

```{r}
general_22 <- general_22 |>
  mutate(
    pct_moore = (moore/total_votes)*100
  )
```

#now arragne#

```{r}
general_22 <- general_22 |>
  mutate(
    pct_moore = (moore/total_votes)*100
  ) |> 
  arrange(desc(pct_moore))
```

#now results ordered by pct_moore with highest % 1st; reverse is ofc with arrange function#

#mutate is also useful for standardizing data -- ex, making different spellings easier to comb through#

#when there is a mix of styles (lower-case and upper-case etc.) R will think of these as different names and try to organize the data accordingly: to fix! can use function -- str_to_upper -- which will convert character column into all uppercase: can be used for addresses, zip codes, anything that is misspelled

```{r}
standardized_maryland_expenses <- maryland_expenses |>
  mutate(
    payee_upper = str_to_upper(payee_name)
)
```


#mutate more powerful when combined with other functions# 

ex
```{r}
maryland_expenses_with_state <- maryland_expenses |>
  mutate(
    state = case_when(
        str_detect(address, " Maryland ") ~ "MD",
        str_detect(address, " California ") ~ "CA",
        str_detect(address, " Washington ") ~ "WA",
        str_detect(address, " Louisiana ") ~ "LA",
        str_detect(address, " Florida ") ~ "FL",
        str_detect(address, " North Carolina ") ~ "NC",
        str_detect(address, " Massachusetts ") ~ "MA",
        str_detect(address, " West Virginia ") ~ "WV",
        str_detect(address, " Virginia ") ~ "VA",
        .default = NA
      )
  )
```

#begins as typical mutate statement, but -- case_when -- introduces each line checks to see if pattern is contained in address column followed by a -- ~ -- and then a value for new column for records that match that check: can be read as: "if we find 'Maryland' in the address column, then put 'MD' in the state column": and if no match --> make state -- NA --#

#thus, new state column can be used to make summarizing easier#

```{r}
maryland_expenses_with_state |>
  group_by(state) |>
  summarize(total = sum(amount)) |>
  arrange(desc(total))
```


Chapter 17: Working with dates

#correct way to display dates: YYYY-MM-DD#

#problem so common that the tidyverse has an entire library for dealing with it -- lubridate -- to solve more common and less tricky problem

```{r}
install.packages('lubridate')
```

#example run through#

```{r}
maryland_expenses <- read_csv("data/maryland_expenses.csv")
```

```{r}
head(maryland_expenses)
```

#the first column --> looks like a date, but the -- <chr -- below the column name, means R thinks it's actually a character column: so need to make into a actual date column, which lubridate is good at doing; has variety of functions that match the format of the data#

#in this case, current format is m/d/y, and the lubridate function is called mdy that can be used with mutate#

```{r}
maryland_expenses <- maryland_expenses |> mutate(expenditure_date=mdy(expenditure_date))
```

```{r}
head(maryland_expenses)
```

#now R will recognize the column as a date column and looks like it should -- YYYY-MM-DD#

#lubridate has functions for almost any type of character date format, mdy, ymd, even datetimes like ymd_hms#

#the -- readr -- anticipates some date formating and can automatically handle many issues (indeed uses lubridate under its hood)#

#when importing CSV file, be sure to use read_csv, not read.csv#

#in spreadsheets can extract portions of dates --a month, day or year-- with formulas: can do same in R with lubridate

#Let’s say we wanted to add up the total amount spent in each month in our Maryland expenses data.

We could use formatting to create a Month field but that would group all the Aprils ever together. We could create a year and a month together, but that would give us an invalid date object and that would create problems later. Lubridate has something called a floor date that we can use.

So to follow along here, we’re going to use mutate to create a month field, group by to lump them together, summarize to count them up and arrange to order them. We’re just chaining things together.#

```{r}
maryland_expenses |>
  mutate(month = floor_date(expenditure_date, "month")) |>
  group_by(month) |>
  summarise(total_amount = sum(amount)) |>
  arrange(desc(total_amount))
```



Chapter 16: Filters and selections

#often have more data then necessary --> sometimes want to be rid of data#

#dplyr --> filtering and selecting#

#Filtering creates a subset of the data based on criteria. All records where the amount is greater than 150,000. All records that match “College Park”. Something like that. Filtering works with rows – when we filter, we get fewer rows back than we start with.#

#Selecting simply returns only the fields named. So if you only want to see city and amount, you select those fields. When you look at your data again, you’ll have two columns. If you try to use one of your columns that you had before you used select, you’ll get an error. Selecting works with columns. You will have the same number of records when you are done, but fewer columns of data to work with.#

#run through an example#

```{r}
library(tidyverse)
```

```{r}
umd_courses <- read_rds("data/umd_courses.rds")
```


#if want to see only those courses offered by particular department, can use -- filter -- function to isolate just those records#

#filter works with something called a comparison operator: need to filter all records equal to "journalism": comparison operators in R, like most programing languages, are == for equal to, != for not equal to, > for greater than, >= for greater than or equal to and so on#

#!!!!!! -- be careful: = is not == -- and = is not "equal to"

= is an assignment operator in most languages - how things are named#

```{r}
journalism_courses <- umd_courses |> filter(department == "Journalism")

head(journalism_courses)
```

#if want to only work with course id and title...#

```{r}
selected_journalism_courses <- journalism_courses |> select(id, title)

head(selected_journalism_courses)
```


#if want to see only all the courses in theatre dept. with at least 15 seats#

#first way; chain together multiple filters#

```{r}
theatre_seats_15 <- umd_courses |> filter(department == "Theatre") |> filter(seats >= 15)

nrow(theatre_seats_15)
```

#gives up 308 records, but a single filter and boolean operators -- AND and OR -- can do this better#

#in this case, AND is & and OR is |#

#difference? with AND --> all conditions must be true to be included#

#difference? with OR --> any of those conditions things can be true and it will be included#

```{r}
and_theatre_seats_15 <- umd_courses |> filter(department == "Theatre" & seats >= 15)

nrow(and_theatre_seats_15)
```

#AND here gives the same answer as the attempt before#

```{r}
and_theatre_seats_15 <- umd_courses |> filter(department == "Theatre" | seats >= 15)

nrow(and_theatre_seats_15)
```

#OR gives 54,000 rows that are EITHER theatre classes OR have at least 15 seats#

#OR is additive; AND is restrictive#

#advice is to use filters one at a time and look at changes rather than doing multiple at once --> easier to see what is working this way#


#to turn off scientific notation in a notebook#

```{r}
# turn off sci notation
options(scipen=999)
```


#book chapters not specified --> pre_lab_03 notes on DATA CLEANING below:#


#guess_max() function as an argument to use the first 10 rows to set the data type#

#can check for problems that occur when loading data with the function: problems(name of dataset)#

#Things that should be characters -- like county, precinct, candidate -- are characters (chr). Things that should be numbers (dbl) -- like votes -- are numbers.#

EXAMPLE FOR MUTATE:

There are some minor problems. The election_day column is a good example. It read in as a number (chr), even though there clearly are numbers in it judging from our initial inspection. Here's why: the original file has a single value in that column that is "5+".

```{r}
texas_precinct_20 |> filter(election_day == "5+")
```

Because this is just one result that's weird, we can fix it by comparing the other votes Castaneda received in Anderson to the county totals for her. The difference should be what that "5+" value should be. I've done those calculations and it turns out that 49 is the actual likely value.

We can fix that pretty easily, by changing that value to "49" using `case_when` and then using `mutate` to make the entire column numeric.

### Task 6: Fix that "election_day" value and change the "election_day" field data type

**Task** Run the following codeblock to update that single row's election day votes to 49 (".default = election_day leaves all the others unchanged), change the data type of the "election_day" field from a character (chr) to a number, and then glimpse the data, to see the change. Add a description of what the mutate code does to your reference notebook.

```{r}

texas_precinct_20 <- texas_precinct_20 |>
  mutate(election_day = case_when(
    election_day == '5+' ~ '49',
    .default = election_day
  ))

texas_precinct_20 <- texas_precinct_20 |> mutate(election_day = as.numeric(election_day))

```

#here the mutate function is turning the data from a character/noun into a row that is deliberately to be read by computer as number -> <dbl>#


#installed library(janitor)#

#clean_names() function from janitor to standardize column names#

#**Task** Run the following codeblock to use the clean_names() function from janitor to standardize column names and then use rename() to change the "x1_id" column. Add a description of what this code does to your reference notebook.

```{r}

cleaned_conowingo <- conowingo |>
  clean_names() |> 
  rename(linenumber = x1_linenumber)

# display the cleaned dataset
cleaned_conowingo

```

#this function cleans the name of the previously poorly named column, and then rename function fully fixed the issue as seen in prelab03#

#**Task** Run the following codeblock to use distinct() to get rid of duplicate rows.#

```{r}

# cleaning function
cleaned_conowingo <- conowingo |>
  clean_names() |> 
  rename(linenumber = x1_linenumber) |> 
  mutate(amount = as.numeric(amount)) |> 
  distinct()
  

# display the cleaned dataset
cleaned_conowingo

```


#**Task** Run the following codeblock to use str_tot_title() to standarize capitalization in the "city" field. How many mispellings of Conowingo remain after running this code? Answer below. Add a description of what this code does to your reference notebook.

**Answer** There are still two mispellings of Conowingo after running this code.#

```{r}

# cleaning function
cleaned_conowingo <- conowingo |>
  clean_names() |> 
  rename(linenumber = x1_linenumber) |> 
  mutate(amount = as.numeric(amount)) |> 
  distinct() |>
  mutate(zip = str_sub(zip, start=1L, end=5L)) |>
  mutate(city = str_to_title(city))
  

# display the cleaned dataset
cleaned_conowingo

```

#**Task** Run the following codeblock to use case_when() to fix misspellings of Conowingo in the "city" field using both the exact match method and the str_detect() method. How many mispellings of Conowingo remain after running this code? Answer below. Add a description of what this code does to your reference notebook.

**Answer** There are no more (0) misspellings of the word Conowingo in this code.#

```{r}

# cleaning function
cleaned_conowingo <- conowingo |>
  clean_names() |> 
  rename(linenumber = x1_linenumber) |> 
  mutate(amount = as.numeric(amount)) |> 
  distinct() |>
  mutate(zip = str_sub(zip, start=1L, end=5L)) |>
  mutate(city = str_to_title(city)) |>
  mutate(city = case_when(
    str_detect(city,"^Conowing") ~ "Conowingo",
    TRUE ~ city
  ))
  

# display the cleaned dataset
cleaned_conowingo

```



#Chapter 19: Data Cleaning Part I: Data smells#

#smell test: (failure to give data smell test can lead to miss stories) done to find common mistakes in data#

#some common mistakes:
-  missing data or missing values
- gaps in data
- wrong type of data
- outliers
- sharp curves
- conflicting information within a dataset
- conflicting information across datasets
- wrongly derived data
- internal inconsistancy 
- external inconcsistancy 
- wrong spatial data
- unusable data, including non-standard abbreviations, ambiguous data, extraneous data, inconsistant data#

#**not all of these data 'smells' are detectable in code#


#wrong type of data can be figured out with output 'readr'#

```{r}
# Remove scientific notation
options(scipen=999)
# Load the tidyverse
library(tidyverse)
```

#This time, we’re going to load the data in a CSV format, which stands for comma separated values and is essentially a fancy structured text file. Each column in the csv is separated – “delimited” – by a comma from the next column.#

#so need to introduce a new argument to our function that reads in the data, read_csv(), called “guess_max”

As R reads in the csv file, it will attempt to make some calls on what “data type” to assign to each field: number, character, date, and so on. The “guess_max” argument says: look at the values in the whatever number of rows we specify before deciding which data type to assign. In this case, we’ll pick 10#

```{r}
payments <- read_csv("data/State_of_Maryland_Payments_Data__FY2008_to_FY2024_20250115.csv", guess_max=10)
```

^^^Warning: One or more parsing issues, call `problems()` on your data frame for details,
e.g.:
  dat <- vroom(...)
  problems(dat)#
  
#^^ advises to run the problems() function to see what is going on that is wrong#
  
```{r}
problems(payments)
```

#produces a table of all the parsing problems. It has 369 rows, which means we have that some problems but not a huge number considering we have 369,000 rows. In almost every case here, the readr library has guessed that a given column was of a “double” data type – a number. It did it based on very limited information – only 10 rows. So, when it hit a value that looked like a date, or a character string, it didn’t know what to do. So it just didn’t read in that value correctly.#

#asy way to fix this is to set the guess_max argument higher. It will take a little longer to load, but we’ll use every single row in the data set to guess the column type – all 322,138 of them.#

```{r}
payments <- read_csv("data/State_of_Maryland_Payments_Data__FY2008_to_FY2024_20250115.csv", guess_max=369008)
```

#no parsing errors this time --> can see what the columns are using the glimpse function#

```{r}
glimpse(payments)
```

#Things that should be characters – like agency name, vendor name – are characters (chr). Things that should be numbers (dbl) – like amount and fiscal year – are numbers. We’ve seen before that sometimes dates aren’t defined as date datatypes by R - we can fix that using lubridate.#


#wrong spatial data#

#spatial data means data that refers to some geography: ZIP codes for example. ZIP codes should only be 5 digits, although there sometimes are ranges that make them 9 digits (this often becomes an common data error)#

#We can check to see if any of the zip codes are less than five characters by using a function called str_length inside a filter#

```{r}
payments |>
  group_by(`Vendor Zip`) |>
  filter(str_length(`Vendor Zip`) < 5) |> 
  summarise(
    count=n()
  ) |>
  arrange(desc(count))
```

#gaps in data and missing data#

#often occur when you have date or time element in your data, but there are other potential gaps, too.#

```{r}
md_grants_loans <- read_csv("data/State_of_Maryland_Grant_and_Loan_Data__FY2009_to_FY2022_20250115.csv")
```

#Each row represents a recipient of state grant or loan, along with information about their location and the state agency that provided the money. When we talk about gaps, often they indicate the administrative rules. Here’s an example: let’s count the number of payments in each category (Grant or Loan) by year in this dataset:#

```{r}
md_grants_loans |> 
  group_by(`Fiscal Year`, Category) |> 
  summarize(count = n()) |> 
  arrange(`Fiscal Year`)
```

#`summarise()` has grouped output by 'Fiscal Year'. You can override using the
`.groups` argument.#

#can see a couple of issues here: first, there is no loan data for FY 2009. That’s mentioned in the source page for the data. It’s good to be aware of all gaps in data, but they don’t always represent a problem. Second, and more problematic, there are a few records where the Category is NA - that data is missing. There also are some inconsistent values - there are 50 records in FY2013 with the category of “L” (probably loans) and one in FY 2017 that is listed as “Contract”.#

#unusual outliers#

#unusual values: Are there any unusually large values or unusually small values? Are there any values that raise immediate questions about the data?#

```{r}
md_grants_loans |> 
  arrange(Amount)
```

#There are two grants for less than $100, which might not be problematic at all, but given that just two of 19,000 are for very small amounts you might wonder if there are suggested amounts for applicants and how tiny ones get evaluated compared to very large requests. As journalists, we should be skeptical of information put in front of us and ask why or what it says about the data itself.#



#Chapter 20: Data Cleaning Part II: Janitor#

#cleaning data is the process of fixing issues in your data so you can answer the questions you want to answer. Data cleaning is a critical step that you can’t skip. A standard metric is that 80 percent of the time working with data will be spent cleaning and verifying data, and 20 percent the more exciting parts like analysis and visualization.

The tidyverse has a lot of built-in tools for data cleaning. We’re also going to make use of a new library, called janitor that has a bunch of great functions for cleaning data.#

```{r}
md_grants_loans <- read_csv("data/State_of_Maryland_Grant_and_Loan_Data__FY2009_to_FY2022_20250115.csv")
```

#are a number of issues with this data set that might get in the way of asking questions and receiving accurate answers:

- The column names have spaces in them. This isn’t a deal-breaker, as we used this dataframe previously. But it does require that you do some things differently when writing code, and ideally you don’t want spaces in your column names.
- Inconsistent capitalization across multiple columns. Sometimes the grantee is capitalized, and other times not. Portions of the grantor name are sometimes capitalized. This issue will ruin your ability to count and add things using those columns.
- The zip field mixes five digit ZIP codes and nine digit ZIP codes, and some of the records include spaces. If we wanted to group and count the number of loans in a given ZIP code, this inconsistency would not let us do that correctly.
- The category column is inconsistent and has some missing values.#

#cleaning headers#

#One of the first places we can start with cleaning data is cleaning the column names (or headers).

Every system has their own way of recording headers, and every developer has their own thoughts of what a good idea is within it. R is most happy when headers are lower case, without special characters.

If column headers start with a number, or have a space in between two words, you have to set them off with backticks when using them in a function. Generally speaking, we want one word (or words separated by an underscore), all lowercase, that don’t start with numbers.#

#the janitor library makes fixing headers trivially simple with the function clean_names()#

```{r}
# cleaning function
cleaned_md_grants_loans <- md_grants_loans |>
  clean_names()

# display the cleaned dataset
cleaned_md_grants_loans
```

#This function changed Zip Code to zip_code and generally got rid of capital letters and replaced spaces with underscores. If we wanted to rename a column, we can use a tidyverse function rename() to do that. Let’s change grantor to source as an example. NOTE: when using rename(), the new name comes first.#

```{r}
# cleaning function
cleaned_md_grants_loans <- md_grants_loans |>
  clean_names() |> 
  rename(source = grantor)

# display the cleaned dataset
cleaned_md_grants_loans
```

#changing capitalization#

#Right now the source, grantee and description columns have inconsistent capitalization. We can fix that using a mutate statement and a function that changes the case of text called str_to_upper(). We’ll use the same columns, overwriting what’s in there since all we’re doing is changing case.#

```{r}
# cleaning function
cleaned_md_grants_loans <- md_grants_loans |>
  clean_names() |> 
  rename(source = grantor) |> 
  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description))

# display the cleaned dataset
cleaned_md_grants_loans
```

#What this does is make it so that using group_by will result in fewer rows due to inconsistent capitalization. It won’t fix misspellings, but working off a single case style definitely helps.#

#duplicates#

#One of the most difficult problems to fix in data is duplicate records in the data. They can creep in with bad joins, bad data entry practices, mistakes – all kinds of reasons. A duplicated record isn’t always there because of an error, but you need to know if it’s there before making that determination.

So the question is, do we have any records repeated?

Here we’ll use a function called get_dupes from the janitor library to check for fully repeated records in our cleaned data set.#

```{r}
cleaned_md_grants_loans |>
  get_dupes()
```

#the answer is … maybe? Because the original dataset doesn’t have a unique identifier for each grant, it’s possible that we have duplicates here, as many as 58. If we could confirm that these actually are duplicates, we can fix this by adding the function distinct() to our cleaning script. This will keep only one copy of each unique record in our table. But we’d need to confirm that first.#

```{r}
# cleaning function
cleaned_md_grants_loans <- md_grants_loans |>
  clean_names() |> 
  rename(source = grantor) |> 
  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description)) |> 
  distinct()

# display the cleaned dataset
cleaned_md_grants_loans
```


#cleaning strings#

#The rest of the problems with this data set all have to do with inconsistent format of values in a few of the columns. To fix these problems, we’re going to make use of mutate() in concert with “string functions” – special functions that allow us to clean up columns stored as character strings. The tidyverse package stringr has lots of useful string functions, more than we’ll learn in this chapter.

Let’s start by cleaning up the zip field. Remember, some of the rows had a five-digit ZIP code, while others had a nine-digit ZIP code, separated by a hyphen or not.

We’re going to write code that tells R to make a new column for our zips, keeping the first five digits on the left, and get rid of anything after that by using mutate() in concert with str_sub(), from the stringr package.#

```{r}
# cleaning function
cleaned_md_grants_loans <- md_grants_loans |>
  clean_names() |> 
  rename(source = grantor) |> 
  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description)) |> 
  distinct() |>
  mutate(zip5 = str_sub(zip_code, start=1L, end=5L))


# display the cleaned dataset
cleaned_md_grants_loans
```

#Let’s break down that last line of code. It says: take the value in each zip column and extract the first character on the left (1L) through the fifth character on the left (5L), and then use that five-digit zip to populate a new zip5 column.

If we arrange the zip5 column we can see that there are some non-digits in there, so let’s make those NA. For that, we’re going to use case_when(), a function that let’s us say if a value meets a certain condition, then change it, and if it doesn’t, don’t change it.#

```{r}
# cleaning function
cleaned_md_grants_loans <- md_grants_loans |>
  clean_names() |> 
  rename(source = grantor) |> 
  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description)) |> 
  distinct() |>
  mutate(zip5 = str_sub(zip_code, start=1L, end=5L)) |>
  mutate(zip5 = case_when(
    zip5 == "Vario" ~ NA,
    zip5 == "UB7 O" ~ NA,
    zip5 == "UB7 " ~ NA,
    .default = zip5
  ))

# display the cleaned dataset
cleaned_md_grants_loans
```

#That last bit is a little complex, so let’s break it down.

What the code above says, in English, is this: Look at all the values in the zip5 column. If the value is “Vario”, then (that’s what the “~” means, then) replace it with NA. Same for the other variations. If it’s anything other than that (that’s what “TRUE” means, otherwise), then keep the existing value in that column.

Instead of specifying the exact value, we can also solve the problem by using something more generalizable, using a function called str_detect(), which allows us to search parts of words.

The second line of our case_when() function below now says, in English: look in the city column. If you find that one of the values starts with “UB7” (the “^” symbol means “starts with”), then (the tilde ~ means then) change it to NA.#

```{r}
# cleaning function
cleaned_md_grants_loans <- md_grants_loans |>
  clean_names() |> 
  rename(source = grantor) |> 
  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description)) |> 
  distinct() |>
  mutate(zip5 = str_sub(zip_code, start=1L, end=5L)) |>
  mutate(zip5 = case_when(
    zip5 == "Vario" ~ NA,
    str_detect(zip5, "^UB7") ~ NA,
    .default = zip5
  ))

# display the cleaned dataset
cleaned_md_grants_loans
```

#We’ve gotten the source and zip code data as clean as we can, and now we can answer our question: which zip code has gotten the most amount of money from the Maryland Tourism Board? A good rule of thumb is that you should only spend time cleaning fields that are critical to the specific analysis you want to do.#

```{r}
cleaned_md_grants_loans |> 
  filter(source == 'COMMERCE/MARYLAND TOURISM BOARD') |> 
  group_by(zip5) |> 
  summarize(total_amount = sum(amount)) |> 
  arrange(desc(total_amount))
```

#why, it's downtown Baltimore, including the Inner Harbor area#





















